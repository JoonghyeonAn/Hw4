Step 1. ì˜ìƒ íŒŒì¼ ì—…ë¡œë“œ
from google.colab import files
import os

# ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
upload_dir = "/content/uploads"
os.makedirs(upload_dir, exist_ok=True)

# ì˜ìƒ íŒŒì¼ ì—…ë¡œë“œ
uploaded = files.upload()

# ì—…ë¡œë“œëœ íŒŒì¼ ê²½ë¡œ ì„¤ì •
uploaded_filename = list(uploaded.keys())[0]
video_path = os.path.join(upload_dir, uploaded_filename)

print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {video_path}")

Step 2. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (YOLOv5 í¬í•¨)
!git clone https://github.com/ultralytics/yolov5.git
%cd yolov5
!pip install -r requirements.txt
%cd /content

Step 3. YOLOv5 ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  ì¤€ë¹„
import torch
import cv2
import numpy as np
from yolov5.models.common import DetectMultiBackend
from yolov5.utils.general import non_max_suppression, scale_boxes
from yolov5.utils.augmentations import letterbox

model = DetectMultiBackend("yolov5s.pt", device='cpu')
stride, names = model.stride, model.names

Step 4. íŠ¸ëž˜í‚¹ + ì†ë„ ê³„ì‚° + ì„ í˜• ê²½ë¡œ ì˜ˆì¸¡ + ì¶©ëŒ ì˜ˆì¸¡ + ë¡œê·¸ ì €ìž¥
# ì´ˆê¸° ì„¤ì •
cap = cv2.VideoCapture(video_path)
fps = cap.get(cv2.CAP_PROP_FPS)
output_path = "/content/output_tracking.mp4"
log_path = "/content/collision_log.txt"

fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = None

prev_positions = {}
log_file = open(log_path, "w")

while True:
    ret, frame = cap.read()
    if not ret:
        break

    img = letterbox(frame, 640, stride=32, auto=True)[0]
    img = img.transpose((2, 0, 1))[::-1]
    img = np.ascontiguousarray(img)
    img_tensor = torch.from_numpy(img).float() / 255.0
    img_tensor = img_tensor.unsqueeze(0)

    pred = model(img_tensor)
    pred = non_max_suppression(pred, 0.25, 0.45, classes=[2, 3, 5, 7])  # ì°¨ëŸ‰ ê´€ë ¨ class

    results = pred[0]
    boxes = results[:, :4]
    for i, box in enumerate(boxes):
        x1, y1, x2, y2 = map(int, box)
        cx = (x1 + x2) // 2
        cy = (y1 + y2) // 2

        # ì°¨ëŸ‰ ID: ì—¬ê¸°ì„  ìž„ì‹œë¡œ ië¡œ ì‚¬ìš© (ì •ì‹ íŠ¸ëž˜ì»¤ë¥¼ ì“°ë©´ ê°œì„  ê°€ëŠ¥)
        vehicle_id = i

        # ì†ë„ ê³„ì‚°
        if vehicle_id in prev_positions:
            px, py = prev_positions[vehicle_id]
            dx = cx - px
            dy = cy - py
            distance = np.sqrt(dx**2 + dy**2)
            speed = distance * fps  # ë‹¨ìœ„: pixel/frame â†’ pixel/sec

            # ë°©í–¥
            direction = np.arctan2(dy, dx)

            # ì„ í˜• ê²½ë¡œ ì˜ˆì¸¡
            future_pos = (cx + dx, cy + dy)

            # ì¶©ëŒ ì‹œê°„ ì˜ˆì¸¡
            for vid2, (px2, py2) in prev_positions.items():
                if vid2 == vehicle_id:
                    continue
                dist = np.sqrt((future_pos[0]-px2)**2 + (future_pos[1]-py2)**2)
                relative_speed = abs(speed - np.linalg.norm([px2-cx, py2-cy])*fps)
                if relative_speed > 0:
                    time_to_collision = dist / relative_speed
                    if time_to_collision < 2.0:
                        msg = f"[âš ï¸ ì¶©ëŒ ì˜ˆìƒ] ì°¨ëŸ‰ {vehicle_id} vs ì°¨ëŸ‰ {vid2} : {time_to_collision:.2f}ì´ˆ í›„"
                        log_file.write(msg + "\n")
                        print(msg)

        prev_positions[vehicle_id] = (cx, cy)
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)
        cv2.putText(frame, f"ID:{vehicle_id}", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)

    if out is None:
        h, w = frame.shape[:2]
        out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))

    out.write(frame)

cap.release()
if out:
    out.release()
log_file.close()

print("ðŸŽ¬ ì™„ë£Œëœ ì˜ìƒ ì €ìž¥: ", output_path)
print("ðŸ“„ ì¶©ëŒ ë¡œê·¸ í™•ì¸: ", log_path)
